{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"font-family:verdana;font-size:300%;text-align:center;background-color:#f2f2f2;color:#0d0d0d\">AMMI NLP - Part 1</h1>\n",
    "\n",
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:Center;color:#993333\"> Lab 1: Introduction to text classification  </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:left;color:blue\">Section 1: Text Classification with Naive Bayes Classifier </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this part you'll implement Naive Bayes classifier to classify the text. You need to build a model that predicts the langauge of the text given the words of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, sys, math, re\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    '''\n",
    "    Parameters:\n",
    "    filename (string): path to file to be read\n",
    "    \n",
    "    Return: \n",
    "    List of tuples (explained in first question)\n",
    "    '''\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    data = []\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        data.append((tokens[0], tokens[1:]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__deu ['Ich', 'würde', 'alles', 'tun,', 'um', 'dich', 'zu', 'beschützen.']\n"
     ]
    }
   ],
   "source": [
    "data = load_data(\"train1.txt\")\n",
    "label, sentence = data[0]\n",
    "print(label, sentence)\n",
    "# Tuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(data):\n",
    "    '''\n",
    "    Parameters:\n",
    "    \n",
    "    data is  list of [(label, words), (label, worlds), ......]\n",
    "    list of tuples in the shape (string, [list of strings]) )\n",
    "    \n",
    "    Returns: \n",
    "    \n",
    "    This function should return a dictionary containing the following:\n",
    "    { \n",
    "    # label_counts (python dictionary): \n",
    "         {label:  no. of times the label appeared },\n",
    "    # word_counts  (dictionary of dictionaries): \n",
    "         {label: {word: no. of times this word appeared with this label }},\n",
    "    # label_total (int): \n",
    "        total number of labels. (size of train data),\n",
    "    # word_total  (python dictionary) total number of words (from the entire corupus) of the particular label:\n",
    "          {label: no.of words}\n",
    "          \n",
    "          }\n",
    "    \n",
    "    '''\n",
    "    label_total = 0\n",
    "    word_total = defaultdict(lambda: 0)\n",
    "    label_counts = defaultdict(lambda: 0)\n",
    "    word_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "\n",
    "    for example in data:\n",
    "        label, sentence = example\n",
    "        label_counts[label] += 1\n",
    "        for word in sentence:\n",
    "            word_total[word] += 1\n",
    "            word_counts[label][word] += 1\n",
    "            \n",
    "    label_total = len(label_counts.keys())\n",
    "#          word_counts[]\n",
    "        \n",
    "        \n",
    "        ## FILL CODE\n",
    "\n",
    "\n",
    "    return {'label_counts': label_counts, \n",
    "            'word_counts': word_counts, \n",
    "            'label_total': label_total, \n",
    "            'word_total': word_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, mu, label_counts, word_counts, label_total, word_total):\n",
    "    '''\n",
    "     Parameters: \n",
    "        sentence (string): sentence to be classified\n",
    "        mu (positive real number): Laplace Smoothing hyperparameter\n",
    "        ** The other parameters introduced in the count_words function\n",
    "    \n",
    "    Returns:\n",
    "    best_label (string): the label that has the highest score. \n",
    "    \n",
    "    Implement the function to predict the best label for the given sentence using Naive Bayes algorithm \n",
    "    \n",
    "    '''\n",
    "    best_label = None\n",
    "    best_score = float('-inf')\n",
    "\n",
    "    for label in word_counts.keys():\n",
    "        score = 0.0\n",
    "        ## FILL CODE\n",
    "        v_size = len(word_counts[label])\n",
    "        for w in sentence:\n",
    "            wc = word_counts[label][w] + mu\n",
    "            tc = word_total[label] + mu*v_size\n",
    "            score += math.log(wc/tc)\n",
    "            \n",
    "        if score > best_score:\n",
    "            best_label =label\n",
    "            best_score = score\n",
    "            \n",
    "        \n",
    "\n",
    "    return best_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts, word_counts, label_total, word_total = list(count_words(data).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__label__fra'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence = 'je suis tres optimiste des prochaines elections'.split()\n",
    "label_counts, word_counts, label_total, word_total = list(count_words(data).values())\n",
    "sentence = 'je'.split()\n",
    "mu = 0.4\n",
    "my_best_label = predict(sentence, mu, label_counts, word_counts, label_total, word_total)\n",
    "my_best_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(valid_data, mu, counts):\n",
    "    '''\n",
    "    Parameters:\n",
    "    valid_data (list of tuples): returned value of load_data function \n",
    "    mu (positive real): Laplace smoothing hyper-parameter\n",
    "    counts (dictionary of dictionaries): return value of count_words_function\n",
    "    \n",
    "    Returns: \n",
    "    accuracy (float): the accuracy of the Naive Bayes classifier\n",
    "    '''\n",
    "\n",
    "    accuracy = 0.0\n",
    "    for label, sentence in valid_data:\n",
    "        ## FILL CODE\n",
    "        label_counts = counts['label_counts']\n",
    "        word_counts = counts['word_counts']\n",
    "        label_total = counts['label_total']\n",
    "        word_total = counts['word_total']\n",
    "        my_best_label = predict(sentence, mu, label_counts, word_counts, label_total, word_total)\n",
    "        if my_best_label == label:\n",
    "            accuracy += 1\n",
    "            \n",
    "    accuracy /= len(valid_data)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Naive Bayes **\n",
      "\n",
      "Validation accuracy: 0.935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"** Naive Bayes **\")\n",
    "print(\"\")\n",
    "\n",
    "mu = 1.0\n",
    "train_data = load_data(\"train1.txt\")\n",
    "valid_data = load_data(\"valid1.txt\")\n",
    "counts = count_words(train_data)\n",
    "\n",
    "print(\"Validation accuracy: %.3f\" % compute_accuracy(valid_data, mu, counts))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:left;color:black\">***************************************************************</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:left;color:blue\">Section 2: Softmax Classification of Text  </h1>\n",
    "\n",
    "##### In this part you'll implement a Softmax Classifier to classify the text (think of it as a 1 layer feedforward neural network). You need to build a model that predicts the langauge of the text given the words of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(filename, threshold=1):\n",
    "    '''\n",
    "    Parameters:\n",
    "    filename (string): path to the data file\n",
    "    \n",
    "    Returns:\n",
    "    word_dic: dictionary maps words to number of times it appeard in the corpus\n",
    "            dic {word: no of times word appears }\n",
    "    label_dic: dictionary maps labels to integers\n",
    "        dic {label: label_id}\n",
    "    '''\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    word_dict, label_dict = {}, {}\n",
    "    counts = defaultdict(lambda: 0)\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        label = tokens[0]\n",
    "\n",
    "        if not label in label_dict:\n",
    "            label_dict[label] = len(label_dict)\n",
    "\n",
    "        for w in tokens[1:]:\n",
    "            counts[w] += 1\n",
    "            \n",
    "    for k, v in counts.items():\n",
    "        if v > threshold:\n",
    "            word_dict[k] = len(word_dict)\n",
    "    return word_dict, label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, word_dict, label_dict):\n",
    "    '''\n",
    "    ## This function converts the text to a list of tuples of \n",
    "    [(label_id, word_representation),...]\n",
    "    \n",
    "    Parameters:\n",
    "    filename (string): path to the file which contains the data\n",
    "    word_dict (python dictionary): returned by build_dict() function above.\n",
    "    label_dict (python dictionary): reutrned by build_dict() function above() \n",
    "    \n",
    "    Returns:\n",
    "    data (list of tuples): \n",
    "    The representation of the data in the form \n",
    "    [(y_0, x_0), .. (y_i, x_i), ... (y_n, x_n))]\n",
    "    where y is the value of the class \n",
    "    x is the representation of the sentence as a word count vector \n",
    "    \n",
    "    '''\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    data = []\n",
    "    dim = len(word_dict)\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        label = tokens[0]\n",
    "\n",
    "        yi = label_dict[label]\n",
    "        xi = np.zeros(dim)\n",
    "        for word in tokens[1:]:\n",
    "            if word in word_dict:\n",
    "                wid = word_dict[word]\n",
    "                xi[wid] += 1.0\n",
    "        data.append((yi, xi))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict, label_dict = build_dict(\"train1.txt\", threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_data(\"train1.txt\", word_dict, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([1., 1., 1., ..., 0., 0., 0.])),\n",
       " (0, array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " (1, array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " (0, array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " (2, array([0., 0., 0., ..., 0., 0., 0.]))]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    This function should apply softmax to vector x\n",
    "    \n",
    "    Parameter:\n",
    "    x (numpy array)\n",
    "    Returns: \n",
    "    softmax(x) (numpy array)\n",
    "    \n",
    "    '''\n",
    "    ## FILL CODE\n",
    "    expo = np.exp(x)\n",
    "    expo_sum = expo.sum()\n",
    "    softmax = expo/expo_sum\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Hint) Derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:center;background-color:#f2f2f2;color:#993333; border:2px; border-style:solid; border-color:gray; padding: 1em\"> \n",
    "   Let $x_i$ be the input vector $W$ is the weight vector $m=$no. of labels, $n=$vocab size\n",
    "    $$ {\\bf S} = W × x_i $$  $x_{i} \\in R^{nx1} W \\in R^{m×n}$\n",
    "    $$  $$\n",
    "    $${\\bf O} = softmax(s) $$\n",
    "    $${\\bf L} = -log(O[y_{i}]) $$\n",
    "    $$  $$\n",
    "    $$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial S} . \\frac{\\partial S}{\\partial W} $$\n",
    "    $$  $$\n",
    "    $ \\nabla L_{W} = (O-y_{true})$  x   $x_{i}^{T} $  \n",
    "    $$ (O-y_{true}) \\in R^{mx1}, x_{i}  \\in R^{nx1}$$\n",
    "\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, data, niter):\n",
    "    '''\n",
    "    This function should perform the Stochastic Gradient Descent algorithm \n",
    "    \n",
    "    Parameter:\n",
    "    w (numpy array): weight vector\n",
    "    data (list of tuples): [...(y_i, x_i)...] from above\n",
    "    niter (int): number of iterations\n",
    "    \n",
    "    Retunrs:\n",
    "    w (numpy array): weight vector after training\n",
    "    '''\n",
    "    nlabels, dim = w.shape\n",
    "    for iter in range(niter):\n",
    "        ## FILL CODE\n",
    "        for label, x in data:\n",
    "            s = np.dot(w, x)\n",
    "            big_O =softmax(s)\n",
    "            y = np.zeros_like(s)\n",
    "            y[label] = 1.0\n",
    "            y = y.reshape(nlabels,1)\n",
    "            big_O = big_O.reshape(nlabels,1)\n",
    "            x = x.reshape(1,dim)\n",
    "#             print(x.shape)\n",
    "            delta_L_w = np.dot(big_O - y, x)\n",
    "            w = w - 0.01 * delta_L_w\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    '''\n",
    "    This function should compute and return the prediction. \n",
    "    Parameters:\n",
    "    w (numpy array): trained weight vector\n",
    "    x (numpy array): word count vector\n",
    "    \n",
    "    Returns: \n",
    "    prediction (int): index of the correct prediction (y_i)\n",
    "    '''\n",
    "    ## FILL CODE\n",
    "    s = np.dot(w, x)\n",
    "    big_O = softmax(s)\n",
    "    prediction = np.argmax(big_O)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(w, valid_data):\n",
    "    '''\n",
    "    This function should compute the accuracy of the classifier \n",
    "    Parameters:\n",
    "    w (numpy array): trained weight vector\n",
    "    valid_data (list of tuples): loaded validation data using load_data() function \n",
    "    \n",
    "    Returns: \n",
    "    accuracy (float): accuracy of the classifier \n",
    "    '''\n",
    "    ## FILL CODE\n",
    "    accuracy = 0.0\n",
    "    for label, x in valid_data:\n",
    "        y = predict(w, x)\n",
    "        if y == label:\n",
    "            accuracy += 1\n",
    "    accuracy /= len(valid_data)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Logistic Regression **\n",
      "\n",
      "\n",
      "Validation accuracy: 0.889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"** Logistic Regression **\")\n",
    "print(\"\")\n",
    "\n",
    "word_dict, label_dict = build_dict(\"train1.txt\")\n",
    "train_data = load_data(\"train1.txt\", word_dict, label_dict)\n",
    "valid_data = load_data(\"valid1.txt\", word_dict, label_dict)\n",
    "\n",
    "nlabels = len(label_dict)\n",
    "dim = len(word_dict)\n",
    "w = np.zeros([nlabels, dim])\n",
    "w = sgd(w, train_data, 5)\n",
    "print(\"\")\n",
    "print(\"Validation accuracy: %.3f\" % compute_accuracy(w, valid_data))\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
